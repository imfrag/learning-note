# 第三章	k近邻法

​	k近邻法（k-nearest neighbor, k-NN），由1968年由Cover和Hart提出，是一种基本分类与回归方法。k值得选择、距离度量及分类决策规则是k近邻法的三个基本要素。

## 3.1 k近邻算法

​	给定一个数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为该类。因此，k近邻法不具有显示的学习过程。

## 3.2 k近邻模型

### 3.2.1 模型

​	k近邻法中，当训练集、距离度量、k值及分类决策规则确定后，则模型是唯一确定的，即对任意一个新的实例，对应分类结果也是唯一确定的。特征空间中，对每个训练实例点，距离该点比其他点更近的所有点构成的区域，称为单元（cell）。k近邻法实质是对特征空间的划分。

### 3.2.2 距离度量

​	k近邻法的特征空间一般是n维实数向量空间$R^n$。使用的距离是欧氏距离，也可采取更一般的$L_p$距离（$L_p$ distance）或Minkowski距离。

> ​	Minkowski距离，闵式距离。（闵氏空间，指狭义相对论中由一个时间维和三个空间维组成的时空），闵式距离定义为：
> $$
> D(x, y) = (\sum_{u=1}^{n}|x_u - y_u|^p)^{\frac{1}{p}}
> $$
> ​	其中$p\geq1$。
>
> 注意：
>
> 1. 闵式距离与特征参数的量纲有关，有不用量纲的特征参数的闵式距离常常是无意义的。
> 2. 闵式距离没有考虑特征参数见的相关性，而马哈拉诺比斯距离解决了这个问题。

> ​	:question:Mahalanobis distance，马哈拉诺比斯距离，表示数据的协方差距离。是一种有效的计算两个未知样本集相似度的方法。与欧氏距离不同的是它考虑到各种特性之间的联系并且是尺度无关的。
>
> ​	对于一个均值为$\mu=(\mu_1,\mu_2, ... ,\mu_p)^T$，协方差矩阵为$\sum$的多变量向量$x=(x_1,x_2,...,x_p)^T$，其马氏距离为：
> $$
> D_M(x)=\sqrt{(x-\mu)^T\sum {^{-1}}(x-\mu)}
> $$
> ​	马哈拉诺比斯距离也可定义为两个服从同一分布并且协方差矩阵$\sum$的随机变量$\vec x$与$\vec y$的差异程度：
> $$
> d(\vec x, \vec y)=\sqrt{(\vec x - \vec y)^T\sum{^{-1}}(\vec x - \vec y)}
> $$
> ​	如果协方差矩阵为单位矩阵，马哈拉诺比斯距离就简化为欧氏距离；如果协方差距真为对角阵，其也可称为正规化的欧氏距离：
> $$
> d(\vec x, \vec y)=\sqrt{\sum_{i=1}^{p}\frac{(x_i-y_i)^2}{\sigma_i^2}}
> $$
> ​	其中$\sigma_i$是$x_i$的标准差。

​	p = 1时，称为曼哈顿距离（Manhattan distance）：
$$
L_1(x_i,x_j)=\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|
$$
​	p = 2时，称为欧氏距离（Euclidean distance）：
$$
L_2(x_i,x_j) = (\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|)^{\frac{1}{2}}
$$
​	$p \to \infin$时，它是各个坐标距离的最大值，称为切比雪夫距离（Chebyshev distance）：
$$
L_{\infin}(x_i, x_j)={max}_{l}|x_i^{(l)}-x_j^{(l)}|
$$

### 3.2.3 k值选择:question:

​	如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差（approximation error）会减小，但缺点是“学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k值得减小就以为这整体模型变得复杂，容易发生过拟合。

​	如果选择较大的k值，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。同时，距离输入实例较远的训练实例也会对预测产生影响。k值得增大就意味着模型变得简单。

​	因此，在应用中，k值一般取一个比较小的数值。通常采用交叉验证发来选择最优的k值。

### 3.2.4 分类决策规则

​	k近邻法中的分类决策规则往往是多数表决（majority voting rule），即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类别。



## 3.3 k近邻法的实现：kd树

​	实现k近邻法时，主要考虑的问题是如何对训练数据进行快速的k近邻搜索。为了提高k近邻搜索的效率，可以采用kd树（kd tree）方法。

> kd树是存储k维空间数据的树结构，这里的k与k近邻法的k意义不同。

### 3.3.1 构造kd树

​	kd树是二叉树，表示对k为空间的一个划分（partition）。构造kd树相当于不断地用垂直于坐标轴的超平面将k为空间切分，构成一些利的k维超矩形区域。kd树的每个节点对应于一个超矩形区域。

### 3.3.2 搜索kd树