# 第6章	逻辑斯蒂回归与最大熵模型

​	逻辑斯蒂回归（logistic regression）是统计学习中的经典分类算法。最大熵是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型（maximum entropy model）。逻辑斯蒂回归模型与最大熵模型都属于对数线性模型。

## 6.1	逻辑斯蒂回归模型

### 6.1.1	逻辑斯谛分布

#### 	定义 6.1 （逻辑斯谛分布）设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有下列分布函数和密度函数：

$$
F(x) = P(X\leq x)=\frac{1}{1+e^{-(x-\mu)/\gamma}} \\
f(x) = F'(x) =\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}
$$

​	其中，$\mu$为未知参数，$\gamma$为形状参数。

### 6.1.2	二项逻辑斯蒂回归模型

​	二项逻辑斯蒂回归模型（binomial logistic regression model）是一种分类模型，有条件概率分布$P(Y|X)$表示，形式为参数化的逻辑斯谛分布。

#### 	定义 6.2 （逻辑斯蒂回归模型）二项逻辑斯蒂回归模型是如下的条件概率分布：

$$
P(Y=1|x) = \frac{exp(wx+b)}{1+exp(wx+b)} \\
P(Y=0|x) = \frac{1}{1+exp(wx+b)}
$$

​	令$\hat w = [w, b]$，$\hat x = [x, 1]$，则上式条件概率分布表示为：
$$
P(Y=1|x) = \frac{exp(\hat w \hat x)}{1+exp(\hat w \hat x)} \\
P(Y=0|x) = \frac{1}{1+exp(\hat w \hat x)}
$$

> 给定一个事件，其发生的概率为$p$，则该事件的==几率==是指该事件发生的概率与该事件不发生的概率的比值。则几率为$\frac{p}{1-p}$。该事件的对数几率（log odds）或logit函数是
> $$
> logit(p) = log\frac{p}{1-p}
> $$
> 对于逻辑斯蒂回归模型
> $$
> logit(P(Y=1|x)) = log\frac{P(Y=1|x)}{1-P(Y=1|x)} = \hat w\hat x
> $$
> 即在逻辑斯蒂回归模型中，输出$Y=1$的对数几率是输入$x$的线性函数。

